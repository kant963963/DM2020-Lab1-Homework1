{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:呂佳勳\n",
    "\n",
    "Student ID:109062570\n",
    "\n",
    "GitHub ID:kant963963"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2020-Lab1-Master Repo](https://github.com/fhcalderon87/DM2020-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2020-Lab1-Master Repo](https://github.com/fhcalderon87/DM2020-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (Oct. 22th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#init\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \\\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# construct dataframe from a list\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])\n",
    "\n",
    "# add category to the dataframe\n",
    "X['category'] = twenty_train.target\n",
    "# add category label also\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, twenty_train))\n",
    "\n",
    "X_sample = X.sample(n=1000) #random state\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# takes a like a minute or two to process\n",
    "X['unigrams'] = X['text'].apply(lambda x: dmh.tokenize_text(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X.text)\n",
    "analyze = count_vect.build_analyzer()\n",
    "\n",
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names()[0:20]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(X.index)[0:20]]\n",
    "plot_z = X_counts[0:20, 0:20].toarray()\n",
    "import seaborn as sns\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, vmax=1, annot=True)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "X_reduced = PCA(n_components = 2).fit_transform(X_counts.toarray())\n",
    "\n",
    "# note this takes time to compute. You may want to reduce the amount of terms you want to compute frequencies for\n",
    "term_frequencies = []\n",
    "for j in range(0,X_counts.shape[1]):\n",
    "    term_frequencies.append(sum(X_counts[:,j].toarray()))\n",
    "term_frequencies = np.asarray(X_counts.sum(axis=0))[0]\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300], \n",
    "            y=term_frequencies[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);\n",
    "\n",
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(X.category)\n",
    "X['bin_category'] = mlb.transform(X['category']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** >>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Answer here\n",
    "X.loc[lambda X: X['category_name']=='comp.graphics', :][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.   \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NA_dict = [{ 'id': 'A', 'missing_example': np.nan },\n",
    "           { 'id': 'B'                    },\n",
    "           { 'id': 'C', 'missing_example': 'NaN'  },\n",
    "           { 'id': 'D', 'missing_example': 'None' },\n",
    "           { 'id': 'E', 'missing_example':  None  },\n",
    "           { 'id': 'F', 'missing_example': ''     }]\n",
    "\n",
    "NA_df = pd.DataFrame(NA_dict, columns = ['id','missing_example'])\n",
    "NA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NA_df['missing_example'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing_example of C is \"NaN\" which is a string not real NaN\n",
    "The missing_example of D is \"None\" which is a string not real NaN\n",
    "The missing_example of F is \"\" which is a string not real NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes to the `X` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X doesnt have any change\n",
    "X.sample will return a new list and this method will not change the value of X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 8 (take home):** \n",
    "We can also do a side-by-side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Below we show you an snapshot of the type of chart we are looking for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://i.imgur.com/9eO431H.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "upper_bound = max(X.category_name.value_counts()) + 40\n",
    "\n",
    "# plot barchart\n",
    "\n",
    "a = pd.concat([X.category_name.value_counts(),X_sample.category_name.value_counts()],\n",
    "              axis=1,join='outer').plot(kind = 'bar',\n",
    "                                        title = 'Category distribution',\n",
    "                                        ylim = [0, upper_bound], \n",
    "                                        rot = 0, fontsize = 12, figsize = (8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 10 (take home):**\n",
    "We said that the `1` at the beginning of the fifth record represents the `00` term. Notice that there is another 1 in the same record. Can you provide code that can verify what word this 1 represents from the vocabulary. Try to do this as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "count_vect.get_feature_names()[X_counts[4,0:100].nonzero()[1][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 11 (take home):** \n",
    "From the chart above, we can see how sparse the term-document matrix is; i.e., there is only one terms with frequency of `1` in the subselection of the matrix. By the way, you may have noticed that we only selected 20 articles and 20 terms to plot the histrogram. As an excersise you can try to modify the code above to plot the entire term-document matrix or just a sample of it. How would you do this efficiently? Remember there is a lot of words in the vocab. Report below what methods you would use to get a nice and useful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "plot_x = [str(i) for i in count_vect.get_feature_names()[0:2000]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(X.index)[0:20]]\n",
    "plot_z = X_counts[0:2000,0:2000].toarray()\n",
    "df_todraw = pd.DataFrame(plot_z).T\n",
    "df_todraw['term'] = plot_x\n",
    "plt.subplots(figsize=(200, 5))\n",
    "sns.barplot(data=df_todraw, \n",
    "            x='term', \n",
    "            y=5).set_xticklabels(df_todraw['term'],rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 12 (take home):\n",
    "Please try to reduce the dimension to 3, and plot the result use 3-D plot. Use at least 3 different angle (camera position) to check your result and describe what you found.\n",
    "\n",
    "$Hint$: you can refer to Axes3D in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X_reduced = PCA(n_components = 3).fit_transform(X_counts.toarray())\n",
    "\n",
    "col = ['coral', 'blue', 'black', 'm']\n",
    "fig = plt.figure(figsize=plt.figaspect(0.25))\n",
    "ax1 = fig.add_subplot(1,1,1,projection='3d')\n",
    "ax2 = fig.add_subplot(1,2,1,projection='3d')\n",
    "ax3 = fig.add_subplot(2,1,1,projection='3d')\n",
    "\n",
    "for c, category in zip(col, categories):\n",
    "    xs = X_reduced[X['category_name'] == category].T[0]\n",
    "    ys = X_reduced[X['category_name'] == category].T[1]\n",
    "    zs = X_reduced[X['category_name'] == category].T[2]\n",
    "   \n",
    "    ax1.scatter(xs, ys, zs, c = c, marker='o')\n",
    "    ax2.scatter(xs, ys, zs, c = c, marker='o')\n",
    "    ax3.scatter(xs, ys, zs, c = c, marker='o')\n",
    "    \n",
    "ax1.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax1.set_xlabel('\\nX Label')\n",
    "ax1.set_ylabel('\\nY Label')\n",
    "ax1.set_zlabel('\\nZ Label')\n",
    "ax2.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax2.set_xlabel('\\nX Label')\n",
    "ax2.set_ylabel('\\nY Label')\n",
    "ax2.set_zlabel('\\nZ Label')\n",
    "ax3.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax3.set_xlabel('\\nX Label')\n",
    "ax3.set_ylabel('\\nY Label')\n",
    "ax3.set_zlabel('\\nZ Label')\n",
    "\n",
    "ax1.view_init(elev=0,azim=0)\n",
    "ax2.view_init(elev=0,azim=90)\n",
    "ax3.view_init(elev=90,azim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 13 (take home):**\n",
    "If you want a nicer interactive visualization here, I would encourage you try to install and use plotly to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import plotly as plty\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "pyplt = plty.offline.plot\n",
    "data = [go.Histogram(x=count_vect.get_feature_names()[:300],y=term_frequencies[:300])] \n",
    "pyplt(data,filename='temp-plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 14 (take home):** \n",
    "The chart above contains all the vocabulary, and it's computationally intensive to both compute and visualize. Can you efficiently reduce the number of terms you want to visualize as an exercise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "a = [[count_vect.get_feature_names()[i],term_frequencies[i]] for i in range(100,200)]\n",
    "a = sorted(a, key=lambda x:x[1], reverse=True)\n",
    "x = [a[i][0] for i in range(100)]\n",
    "y = [a[i][1] for i in range(100)]\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=x, \n",
    "                y=y)\n",
    "g.set_xticklabels(count_vect.get_feature_names()[100:200], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 15 (take home):** \n",
    "Additionally, you can attempt to sort the terms on the `x-axis` by frequency instead of in alphabetical order. This way the visualization is more meaninfgul and you will be able to observe the so called [long tail](https://en.wikipedia.org/wiki/Long_tail) (get familiar with this term since it will appear a lot in data mining and other statistics courses). see picture below\n",
    "\n",
    "![alt txt](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1000px-Long_tail.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "a = [[count_vect.get_feature_names()[i],term_frequencies[i]] for i in range(300)]\n",
    "a = sorted(a, key=lambda x:x[1], reverse=True)\n",
    "x = [a[i][0] for i in range(300)]\n",
    "y = [a[i][1] for i in range(300)]\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=x, \n",
    "                y=y)\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 16 (take home):**\n",
    "Try to generate the binarization using the `category_name` column instead. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "mlb.fit(X.category_name)\n",
    "X['bin_category'] = mlb.transform(X['category_name']).tolist()\n",
    "X[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "Now let us begin to explore the data. The original dataset can be found on the link provided above or you can directly use the version provided by scikit learn. Here we will use the scikit learn version. \n",
    "\n",
    "In this demonstration we are only going to look at 4 categories. This means we will not make use of the complete dataset, but only a subset of it, which includes the 4 categories defined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation\n",
    "So we want to explore and understand our data a little bit better. Before we do that we definitely need to apply some transformations just so we can have our dataset in a nice format to be able to explore it freely and more efficient. Lucky for us, there are powerful scientific tools to transform our data into that tabular format we are so farmiliar with. So that is what we will do in the next section--transform our data into a nice table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "categories = ['amazon_cells', 'imdb', 'yelp']\n",
    "\n",
    "f = open(\"sentiment labelled sentences/amazon_cells_labelled.txt\",encoding=\"utf-8\")\n",
    "a_lines = f.readlines()\n",
    "f.close()\n",
    "f = open('sentiment labelled sentences/imdb_labelled.txt',encoding=\"utf-8\")\n",
    "i_lines = f.readlines()\n",
    "f.close()\n",
    "f = open('sentiment labelled sentences/yelp_labelled.txt',encoding=\"utf-8\")\n",
    "y_lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[a_lines[i][:-3],0,'amazon_cells',a_lines[i][-2]] for i in range(len(a_lines))]\n",
    "a += [[i_lines[i][:-3],1,'imdb',i_lines[i][-2]] for i in range(len(i_lines))]\n",
    "a += [[y_lines[i][:-3],2,'yelp',y_lines[i][-2]] for i in range(len(y_lines))]\n",
    "datasets = pd.DataFrame(a, columns=['sentence','category','category_name','score'])\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise1\n",
    "for t in datasets['sentence'][:3]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise2\n",
    "datasets.loc[lambda X:X['category_name']=='imdb', :][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exercise3\n",
    "datasets.loc[lambda f:f.category_name == 'yelp'].iloc[::10][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Mining using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some serious work now. Let's learn to program some of the ideas and concepts learned so far in the data mining course. This is the only way we can be convince ourselves of the true power of Pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us consider that our dataset has some *missing values* and we want to remove those values. In its current state our dataset has no missing values, but for practice sake we will add some records with missing values and then write some code to deal with these objects that contain missing values. You will see for yourself how easy it is to deal with missing values once you have your data transformed into a Pandas dataframe.\n",
    "\n",
    "Before we jump into coding, let us do a quick review of what we have learned in the Data Mining course. Specifically, let's review the methods used to deal with missing values.\n",
    "\n",
    "The most common reasons for having missing values in datasets has to do with how the data was initially collected. A good example of this is when a patient comes into the ER room, the data is collected as quickly as possible and depending on the conditions of the patients, the personal data being collected is either incomplete or partially complete. In the former and latter cases, we are presented with a case of \"missing values\". Knowing that patients data is particularly critical and can be used by the health authorities to conduct some interesting analysis, we as the data miners are left with the tough task of deciding what to do with these missing and incomplete records. We need to deal with these records because they are definitely going to affect our analysis or learning algorithms. So what do we do? There are several ways to handle missing values, and some of the more effective ways are presented below (Note: You can reference the slides - Session 1 Handout for the additional information).\n",
    "\n",
    "- **Eliminate Data Objects** - Here we completely discard records once they contain some missing values. This is the easiest approach and the one we will be using in this notebook. The immediate drawback of going with this approach is that you lose some information, and in some cases too much of it. Now imagine that half of the records have at least one or more missing values. Here you are presented with the tough decision of quantity vs quality. In any event, this decision must be made carefully, hence the reason for emphasizing it here in this notebook. \n",
    "\n",
    "- **Estimate Missing Values** - Here we try to estimate the missing values based on some criteria. Although this approach may be proven to be effective, it is not always the case, especially when we are dealing with sensitive data, like **Gender** or **Names**. For fields like **Address**, there could be ways to obtain these missing addresses using some data aggregation technique or obtain the information directly from other databases or public data sources.\n",
    "\n",
    "- **Ignore the missing value during analysis** - Here we basically ignore the missing values and proceed with our analysis. Although this is the most naive way to handle missing values it may proof effective, especially when the missing values includes information that is not important to the analysis being conducted. But think about it for a while. Would you ignore missing values, especially when in this day and age it is difficult to obtain high quality datasets? Again, there are some tradeoffs, which we will talk about later in the notebook.\n",
    "\n",
    "- **Replace with all possible values** - As an efficient and responsible data miner, we sometimes just need to put in the hard hours of work and find ways to makes up for these missing values. This last option is a very wise option for cases where data is scarce (which is almost always) or when dealing with sensitive data. Imagine that our dataset has an **Age** field, which contains many missing values. Since **Age** is a continuous variable, it means that we can build a separate model for calculating the age for the incomplete records based on some rule-based appraoch or probabilistic approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we are going to go with the first option but you may be asked to compute missing values, using a different approach, as an exercise. Let's get to it!\n",
    "\n",
    "First we want to add the dummy records with missing values since the dataset we have is perfectly composed and cleaned that it contains no missing values. First let us check for ourselves that indeed the dataset doesn't contain any missing values. We can do that easily by using the following built-in function provided by Pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(datasets.isnull())\n",
    "datasets.isnull().apply(lambda x: dmh.check_missing_values(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise4\n",
    "datasets.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values\n",
    "dummy_series = pd.Series([\"dummy_record\", 1, 1], index=[\"sentence\", \"category\", \"score\"])\n",
    "result_with_series = datasets.append(dummy_series, ignore_index=True)\n",
    "print(len(result_with_series))\n",
    "result_with_series.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy record as dictionary format\n",
    "dummy_dict = [{'sentence': 'dummy_record',\n",
    "               'category': 1,\n",
    "               'score': 1\n",
    "              }]\n",
    "datasets = datasets.append(dummy_dict, ignore_index=True)\n",
    "print(len(datasets))\n",
    "print(datasets.isnull().apply(lambda x: dmh.check_missing_values(x)))\n",
    "datasets.dropna(inplace=True)\n",
    "print(datasets.isnull().apply(lambda x: dmh.check_missing_values(x)))\n",
    "len(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with Duplicate Data\n",
    "Dealing with duplicate data is just as painful as dealing with missing data. The worst case is that you have duplicate data that has missing values. But let us not get carried away. Let us stick with the basics. As we have learned in our Data Mining course, duplicate data can occur because of many reasons. The majority of the times it has to do with how we store data or how we collect and merge data. For instance, we may have collected and stored a tweet, and a retweet of that same tweet as two different records; this results in a case of data duplication; the only difference being that one is the original tweet and the other the retweeted one. Here you will learn that dealing with duplicate data is not as challenging as missing values. But this also all depends on what you consider as duplicate data, i.e., this all depends on your criteria for what is considered as a duplicate record and also what type of data you are dealing with. For textual data, it may not be so trivial as it is for numerical values or images. Anyhow, let us look at some code on how to deal with duplicate records in our `X` dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us check how many duplicates we have in our current dataset. Here is the line of code that checks for duplicates; it is very similar to the `isnull` function that we used to check for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dealing with Duplicate Data\n",
    "print(datasets.duplicated())\n",
    "print(sum(datasets.duplicated()))\n",
    "print(sum(datasets.duplicated('sentence')))\n",
    "\n",
    "result_datasets = copy.deepcopy(datasets)\n",
    "result_datasets.drop_duplicates(keep=False, inplace=True) # inplace applies changes directly on our dataframe\n",
    "print(len(result_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling\n",
    "datasets_sample = datasets.sample(n=1000) #random state\n",
    "print(len(datasets_sample))\n",
    "print(datasets_sample[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#matplotlib\n",
    "print(datasets.category_name.value_counts())\n",
    "\n",
    "# plot barchart for datasets_sample\n",
    "datasets.category_name.value_counts().plot(kind = 'bar',\n",
    "                                           title = 'Category distribution',\n",
    "                                           ylim = [0, 1100],        \n",
    "                                           rot = 0, fontsize = 11, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  Data Preprocessing\n",
    "In the Data Mining course we learned about the many ways of performing data preprocessing. In reality, the list is quiet general as the specifics of what data preprocessing involves is too much to cover in one course. This is especially true when you are dealing with unstructured data, as we are dealing with in this particular notebook. But let us look at some examples for each data preprocessing technique that we learned in the class. We will cover each item one by one, and provide example code for each category. You will learn how to peform each of the operations, using Pandas, that cover the essentials to Preprocessing in Data Mining. We are not going to follow any strict order, but the items we will cover in the preprocessing section of this notebook are as follows:\n",
    "\n",
    "- Aggregation\n",
    "- Sampling\n",
    "- Dimensionality Reduction\n",
    "- Feature Subset Selection\n",
    "- Feature Creation\n",
    "- Discretization and Binarization\n",
    "- Attribute Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sampling\n",
    "The first concept that we are going to cover from the above list is sampling. Sampling refers to the technique used for selecting data. The functionalities that we use to  selected data through queries provided by Pandas are actually basic methods for sampling. The reasons for sampling are sometimes due to the size of data -- we want a smaller subset of the data that is still representatitive enough as compared to the original dataset. \n",
    "\n",
    "We don't have a problem of size in our current dataset since it is just a couple thousand records long. But if we pay attention to how much content is included in the `text` field of each of those records, you will realize that sampling may not be a bad idea after all. In fact, we have already done some sampling by just reducing the records we are using here in this notebook; remember that we are only using four categories from the all the 20 categories available. Let us get an idea on how to sample using pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(datasets_sample.category_name.value_counts())\n",
    "\n",
    "# plot barchart for X_sample\n",
    "datasets_sample.category_name.value_counts().plot(kind = 'bar',\n",
    "                                                  title = 'Category distribution',\n",
    "                                                  ylim = [0, 400], \n",
    "                                                  rot = 0, fontsize = 12, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise7\n",
    "upper_bound = max(datasets_sample.category_name.value_counts()) + 20\n",
    "\n",
    "# plot barchart for X_sample\n",
    "datasets_sample.category_name.value_counts().plot(kind = 'bar',\n",
    "                                                  title = 'Category distribution',\n",
    "                                                  ylim = [0, upper_bound], \n",
    "                                                  rot = 0, fontsize = 12, figsize = (8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exercise8\n",
    "upper_bound = max(datasets.category_name.value_counts()) + 40\n",
    "\n",
    "# plot barchart\n",
    "\n",
    "a = pd.concat([datasets.category_name.value_counts(),datasets_sample.category_name.value_counts()],\n",
    "              axis=1,join='outer').plot(kind = 'bar',\n",
    "                                        title = 'Category distribution',\n",
    "                                        ylim = [0, upper_bound], \n",
    "                                        rot = 0, fontsize = 12, figsize = (8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Creation\n",
    "The other operation from the list above that we are going to practise on is the so-called feature creation. As the name suggests, in feature creation we are looking at creating new interesting and useful features from the original dataset; a feature which captures the most important information from the raw information we already have access to. In our `X` table, we would like to create some features from the `text` field, but we are still not sure what kind of features we want to create. We can think of an interesting problem we want to solve, or something we want to analyze from the data, or some questions we want to answer. This is one process to come up with features -- this process is usually called `feature engineering` in the data science community. \n",
    "\n",
    "We know what feature creation is so let us get real involved with our dataset and make it more interesting by adding some special features or attributes if you will. First, we are going to obtain the **unigrams** for each text. (Unigram is just a fancy word we use in Text Mining which stands for 'tokens' or 'individual words'.) Yes, we want to extract all the words found in each text and append it as a new feature to the pandas dataframe. The reason for extracting unigrams is not so clear yet, but we can start to think of obtaining some statistics about the articles we have: something like **word distribution** or **word frequency**.\n",
    "\n",
    "Before going into any further coding, we will also introduce a useful text mining library called [NLTK](http://www.nltk.org/). The NLTK library is a natural language processing tool used for text mining tasks, so might as well we start to familiarize ourselves with it from now (It may come in handy for the final project!). In partcular, we are going to use the NLTK library to conduct tokenization because we are interested in splitting a sentence into its individual components, which we refer to as words, emojis, emails, etc. So let us go for it! We can call the `nltk` library as follows:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation\n",
    "\n",
    "# takes a like a minute or two to process\n",
    "datasets['unigrams'] = datasets['sentence'].apply(lambda x: dmh.tokenize_text(x))\n",
    "print(datasets[0:4][\"unigrams\"])\n",
    "print(datasets[0:4])\n",
    "print(list(datasets[0:1]['unigrams']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature subset selection\n",
    "Okay, so we are making some headway here. Let us now make things a bit more interesting. We are going to do something different from what we have been doing thus far. We are going use a bit of everything that we have learned so far. Briefly speaking, we are going to move away from our main dataset (one form of feature subset selection), and we are going to generate a document-term matrix from the original dataset. In other words we are going to be creating something like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vS01RrtPHS3r1Lf8UjX4POgDol-lVF4JAbjXM3SAOU-dOe-MqUdaEMWwJEPk9TtiUvcoSqTeE--lNep/pub?w=748&h=366)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, it won't have the same shape as the table above, but we will get into that later. For now, let us use scikit learn built in functionalities to generate this document. You will see for yourself how easy it is to generate this table without much coding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature subset selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "datasets_counts = count_vect.fit_transform(datasets.sentence)\n",
    "analyze = count_vect.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise9\n",
    "analyze(\" \".join(list(datasets[:1].sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets_counts.shape)\n",
    "print(count_vect.get_feature_names()[0:10])\n",
    "print(datasets_counts[0:5, 0:100].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise10\n",
    "#find the meaning of 1 in third record\n",
    "count_vect.get_feature_names()[datasets_counts[3:4,0:100].nonzero()[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#seaborn\n",
    "plot_x = [\"term_\"+str(i) for i in count_vect.get_feature_names()[0:20]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(datasets.index)[0:20]]\n",
    "plot_z = datasets_counts[0:20, 0:20].toarray()\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise11\n",
    "plot_x = [str(i) for i in count_vect.get_feature_names()[0:2000]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(datasets.index)[0:20]]\n",
    "plot_z = datasets_counts[0:2000,0:2000].toarray()\n",
    "df_todraw = pd.DataFrame(plot_z).T\n",
    "df_todraw['term'] = plot_x\n",
    "plt.subplots(figsize=(200, 5))\n",
    "sns.barplot(data=df_todraw, \n",
    "            x='term', \n",
    "            y=5).set_xticklabels(df_todraw['term'],rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Dimensionality Reduction\n",
    "Since we have just touched on the concept of sparsity most naturally the problem of \"curse of dimentionality\" comes up. I am not going to get into the full details of what dimensionality reduction is and what it is good for just the fact that is an excellent technique for visualizing data efficiently (please refer to notes for more information). All I can say is that we are going to deal with the issue of sparsity with a few lines of code. And we are going to try to visualize our data more efficiently with the results.\n",
    "\n",
    "We are going to make use of Principal Component Analysis to efficeintly reduce the dimensions of our data, with the main goal of \"finding a projection that captures the largest amount of variation in the data.\" This concept is important as it is very useful for visualizing and observing the characteristics of our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA Algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "**Input:** Raw term-vector matrix\n",
    "\n",
    "**Output:** Projections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dimension reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "datasets_reduced = PCA(n_components = 2).fit_transform(datasets_counts.toarray())\n",
    "print(datasets_reduced.shape)\n",
    "\n",
    "col = ['coral', 'blue', 'black', 'm']\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (25,10))\n",
    "ax = fig.subplots()\n",
    "\n",
    "for c, category in zip(col, categories):\n",
    "    xs = datasets_reduced[datasets['category_name'] == category].T[0]\n",
    "    ys = datasets_reduced[datasets['category_name'] == category].T[1]\n",
    "   \n",
    "    ax.scatter(xs, ys, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 12 (take home):¶\n",
    "#Please try to reduce the dimension to 3, and plot the result use 3-D plot. \n",
    "#Use at least 3 different angle (camera position) to check your result and describe what you found.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "datasets_reduced = PCA(n_components = 3).fit_transform(datasets_counts.toarray())\n",
    "\n",
    "col = ['coral', 'blue', 'black', 'm']\n",
    "fig = plt.figure(figsize=plt.figaspect(0.25))\n",
    "ax1 = fig.add_subplot(1,1,1,projection='3d')\n",
    "ax2 = fig.add_subplot(1,2,1,projection='3d')\n",
    "ax3 = fig.add_subplot(2,1,1,projection='3d')\n",
    "\n",
    "for c, category in zip(col, categories):\n",
    "    xs = datasets_reduced[datasets['category_name'] == category].T[0]\n",
    "    ys = datasets_reduced[datasets['category_name'] == category].T[1]\n",
    "    zs = datasets_reduced[datasets['category_name'] == category].T[2]\n",
    "   \n",
    "    ax1.scatter(xs, ys, zs, c = c, marker='o')\n",
    "    ax2.scatter(xs, ys, zs, c = c, marker='o')\n",
    "    ax3.scatter(xs, ys, zs, c = c, marker='o')\n",
    "    \n",
    "ax1.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax1.set_xlabel('\\nX Label')\n",
    "ax1.set_ylabel('\\nY Label')\n",
    "ax1.set_zlabel('\\nZ Label')\n",
    "ax2.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax2.set_xlabel('\\nX Label')\n",
    "ax2.set_ylabel('\\nY Label')\n",
    "ax2.set_zlabel('\\nZ Label')\n",
    "ax3.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax3.set_xlabel('\\nX Label')\n",
    "ax3.set_ylabel('\\nY Label')\n",
    "ax3.set_zlabel('\\nZ Label')\n",
    "\n",
    "ax1.view_init(elev=0,azim=0)\n",
    "ax2.view_init(elev=0,azim=90)\n",
    "ax3.view_init(elev=90,azim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Atrribute Transformation / Aggregation\n",
    "We can do other things with the term-vector matrix besides applying dimensionalaity reduction technique to deal with sparsity problem. Here we are going to generate a simple distribution of the words found in all the entire set of articles. Intuitively, this may not make any sense, but in data science sometimes we take some things for granted, and we just have to explore the data first before making any premature conclusions. On the topic of attribute transformation, we will take the word distribution and put the distribution in a scale that makes it easy to analyze patterns in the distrubution of words. Let us get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute these frequencies for each term in all documents. Visually speaking, we are seeking to add values of the 2D matrix, vertically; i.e., sum of each column. You can also refer to this process as aggregation, which we won't explore further in this notebook because of the type of data we are dealing with. But I believe you get the idea of what that includes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt txt](https://docs.google.com/drawings/d/e/2PACX-1vTMfs0zWsbeAl-wrpvyCcZqeEUf7ggoGkDubrxX5XtwC5iysHFukD6c-dtyybuHnYigiRWRlRk2S7gp/pub?w=750&h=412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Atrribute Transformation / Aggregation\n",
    "term_frequencies = []\n",
    "for j in range(0,datasets_counts.shape[1]):\n",
    "    term_frequencies.append(sum(datasets_counts[:,j].toarray()))\n",
    "term_frequencies = np.asarray(datasets_counts.sum(axis=0))[0]\n",
    "print(term_frequencies[0])\n",
    "\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300], \n",
    "                y=term_frequencies[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 13\n",
    "#If you want a nicer interactive visualization here, I would encourage you try to install and use plotly to achieve this.\n",
    "import plotly as plty\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "pyplt = plty.offline.plot\n",
    "data = [go.Histogram(x=count_vect.get_feature_names()[:300],y=term_frequencies[:300])] \n",
    "pyplt(data,filename='temp-plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 14\n",
    "a = [[count_vect.get_feature_names()[i],term_frequencies[i]] for i in range(100,200)]\n",
    "a = sorted(a, key=lambda x:x[1], reverse=True)\n",
    "x = [a[i][0] for i in range(100)]\n",
    "y = [a[i][1] for i in range(100)]\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=x, \n",
    "                y=y)\n",
    "g.set_xticklabels(count_vect.get_feature_names()[100:200], rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 15\n",
    "a = [[count_vect.get_feature_names()[i],term_frequencies[i]] for i in range(300)]\n",
    "a = sorted(a, key=lambda x:x[1], reverse=True)\n",
    "x = [a[i][0] for i in range(300)]\n",
    "y = [a[i][1] for i in range(300)]\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=x, \n",
    "                y=y)\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "term_frequencies_log = [math.log(i) for i in term_frequencies]\n",
    "\n",
    "plt.subplots(figsize=(100, 10))\n",
    "g = sns.barplot(x=count_vect.get_feature_names()[:300],\n",
    "                y=term_frequencies_log[:300])\n",
    "g.set_xticklabels(count_vect.get_feature_names()[:300], rotation = 90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Discretization and Binarization\n",
    "In this section we are going to discuss a very important pre-preprocessing technique used to transform the data, specifically categorical values, into a format that satisfies certain criteria required by particular algorithms. Given our current original dataset, we would like to transform one of the attributes, `category_name`, into four binary attributes. In other words, we are taking the category name and replacing it with a `n` asymmetric binary attributes. The logic behind this transformation is discussed in detail in the recommended Data Mining text book (please refer to it on page 58). People from the machine learning community also refer to this transformation as one-hot encoding, but as you may become aware later in the course, these concepts are all the same, we just have different prefrence on how we refer to the concepts. Let us take a look at what we want to achieve in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretization and Binarization\n",
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(datasets.category)\n",
    "datasets['bin_category'] = mlb.transform(datasets['category']).tolist()\n",
    "print(datasets[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 16\n",
    "# Try to generate the binarization using the category_name column instead. Does it work?\n",
    "mlb.fit(datasets.category_name)\n",
    "datasets['bin_category'] = mlb.transform(datasets['category_name']).tolist()\n",
    "print(datasets[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=False, stop_words=None, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\", smooth_idf=True, norm='l2')\n",
    "tfidf = vectorizer.fit_transform(datasets.sentence)\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"TFIDF\")\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")  \n",
    "tf = vectorizer.fit_transform(datasets.sentence)\n",
    "df_tf = pd.DataFrame(tf.toarray(),columns=vectorizer.get_feature_names())\n",
    "print(\"CountVector\")\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=False, stop_words=None, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\", smooth_idf=True, norm=None)  \n",
    "X = vectorizer.fit_transform(datasets.sentence)\n",
    "r = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names(),index=categories)\n",
    "print(\"IDF\")\n",
    "idf = vectorizer.idf_\n",
    "pd.DataFrame([vectorizer.idf_], columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf*idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth\n",
    "Q: What are those inefficent parts you noticed? \n",
    "A: heatmap is inefficient due to our data is sparse martrix\n",
    "\n",
    "Q: How can you improve the Data preprocessing for these specific datasets?\n",
    "A: I think delete null data and duplicate data can improve it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
